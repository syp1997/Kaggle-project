{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.models as models\n",
    "from sklearn import preprocessing\n",
    "import zipfile\n",
    "import shutil\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'   \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 427):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarthDataSet(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['sst'])\n",
    "\n",
    "    def __getitem__(self, idx):   \n",
    "        return (self.data['sst'][idx], self.data['t300'][idx], self.data['ua'][idx], self.data['va'][idx]), self.data['label'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_data(data_list, fit=True):\n",
    "    a,b,c,d = data_list[0].shape\n",
    "    all_data = []\n",
    "    for data in data_list:\n",
    "        new_data = data.reshape(-1)\n",
    "        all_data.append(new_data)\n",
    "    all_data = np.stack(all_data,1)\n",
    "    print(all_data.shape)\n",
    "    if fit:\n",
    "        standardScaler.fit(all_data)\n",
    "        print(\"fit train data\")\n",
    "    all_data = standardScaler.transform(all_data)\n",
    "    res_data = []\n",
    "    for i in range(all_data.shape[1]):\n",
    "        data = all_data[:,i].reshape(a,b,c,d)\n",
    "        res_data.append(data)\n",
    "    return res_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # CMIP data    \n",
    "    train = xr.open_dataset('tcdata/enso_round1_train_20210201/CMIP_train.nc')\n",
    "    label = xr.open_dataset('tcdata/enso_round1_train_20210201/CMIP_label.nc')    \n",
    "   \n",
    "    train_sst = train['sst'][:, :12].values.astype('float64')  # (4645, 12, 24, 72)\n",
    "    train_t300 = train['t300'][:, :12].values.astype('float64')\n",
    "    train_ua = train['ua'][:, :12].values.astype('float64')\n",
    "    train_va = train['va'][:, :12].values.astype('float64')\n",
    "    train_label = label['nino'][:, 12:36].values.astype('float64')\n",
    "\n",
    "    train_ua = np.nan_to_num(train_ua) # trans nan to 0\n",
    "    train_va = np.nan_to_num(train_va)\n",
    "    train_t300 = np.nan_to_num(train_t300)\n",
    "    train_sst = np.nan_to_num(train_sst)\n",
    "    \n",
    "#     data_list = [train_sst,train_t300,train_ua,train_va]\n",
    "#     train_sst,train_t300,train_ua,train_va = fit_data(data_list, fit=True)\n",
    "\n",
    "    # SODA data    \n",
    "    train2 = xr.open_dataset('tcdata/enso_round1_train_20210201/SODA_train.nc')\n",
    "    label2 = xr.open_dataset('tcdata/enso_round1_train_20210201/SODA_label.nc')\n",
    "    \n",
    "    train_sst2 = train2['sst'][:, :12].values.astype('float64')  # (100, 12, 24, 72)\n",
    "    train_t3002 = train2['t300'][:, :12].values.astype('float64')\n",
    "    train_ua2 = train2['ua'][:, :12].values.astype('float64')\n",
    "    train_va2 = train2['va'][:, :12].values.astype('float64')\n",
    "    train_label2 = label2['nino'][:, 12:36].values.astype('float64')\n",
    "    \n",
    "    train_sst2 = np.nan_to_num(train_sst2) # trans nan to 0\n",
    "    train_t3002 = np.nan_to_num(train_t3002)\n",
    "    train_ua2 = np.nan_to_num(train_ua2)\n",
    "    train_va2 = np.nan_to_num(train_va2)\n",
    "    \n",
    "#     data_list = [train_sst2,train_t3002,train_ua2,train_va2]\n",
    "#     train_sst2,train_t3002,train_ua2,train_va2 = fit_data(data_list, fit=False)\n",
    "\n",
    "    dict_cmip = {\n",
    "        'sst':train_sst,\n",
    "        't300':train_t300,\n",
    "        'ua':train_ua,\n",
    "        'va': train_va,\n",
    "        'label': train_label}\n",
    "    dict_soda = {\n",
    "        'sst':train_sst2,\n",
    "        't300':train_t3002,\n",
    "        'ua':train_ua2,\n",
    "        'va': train_va2,\n",
    "        'label': train_label2}\n",
    "    \n",
    "    cmip_dataset = EarthDataSet(dict_cmip)\n",
    "    soda_dataset = EarthDataSet(dict_soda)\n",
    "    \n",
    "    train_1, valid_1 = random_split(cmip_dataset, [4545, 100])\n",
    "    train_2, valid_2 = random_split(soda_dataset, [0, 100])\n",
    "    \n",
    "    train_dataset = train_1 \n",
    "    valid_dataset = valid_1\n",
    "    valid_dataset_2 = valid_2\n",
    "    \n",
    "    print('Train samples: {}, Valid1 samples: {}, Valid2 samples: {}'.format(len(train_dataset), len(valid_dataset), len(valid_dataset_2)))\n",
    "    \n",
    "    return train_dataset, valid_dataset, valid_dataset_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 4545, Valid1 samples: 100, Valid2 samples: 100\n"
     ]
    }
   ],
   "source": [
    "set_seed()\n",
    "standardScaler = StandardScaler()\n",
    "train_dataset, valid_dataset, valid_dataset_2 = load_data()      \n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "valid_loader_2 = DataLoader(valid_dataset_2, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coreff(x, y):\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    c1 = sum((x - x_mean) * (y - y_mean))\n",
    "    c2 = sum((x - x_mean)**2) * sum((y - y_mean)**2)\n",
    "    return c1/np.sqrt(c2)\n",
    "\n",
    "def rmse(preds, y):\n",
    "    r = np.sqrt(sum((preds - y)**2) / preds.shape[0])\n",
    "    return r\n",
    "\n",
    "def eval_score(preds, label):\n",
    "    acskill_socre = 0\n",
    "    rmse_score = 0\n",
    "    a = [1.5]*4 + [2]*7 + [3]*7 + [4]*6\n",
    "    for i in range(24):\n",
    "        r = rmse(preds[:, i], label[:, i], ) # T时刻 (100,)\n",
    "        cor = coreff(preds[:, i], label[:, i], )\n",
    "    \n",
    "        rmse_score += r\n",
    "        acskill_socre += a[i] * np.log(i+1) * cor\n",
    "    print(\"acskill_socre:{}, rmse_score:{}\".format(2/3*acskill_socre, rmse_score))\n",
    "    return 2/3 * acskill_socre - rmse_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    best_score = -99\n",
    "    loss_epoch = []\n",
    "    score_epoch = []\n",
    "    score_epoch_2 = []\n",
    "    epoch = -1\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for step, ((sst, t300, ua, va), label) in enumerate(valid_loader):\n",
    "        sst = sst.to(device).float()\n",
    "        t300 = t300.to(device).float()\n",
    "        ua = ua.to(device).float()\n",
    "        va = va.to(device).float()\n",
    "        label = label.to(device).float()\n",
    "        preds = model(sst, t300, ua, va)\n",
    "\n",
    "        y_pred.append(preds)\n",
    "        y_true.append(label)\n",
    "\n",
    "    y_true = torch.cat(y_true, axis=0).cpu().detach().numpy()\n",
    "    y_pred = torch.cat(y_pred, axis=0).cpu().detach().numpy()\n",
    "    x_month = np.arange(24)\n",
    "    score = eval_score(y_true, y_pred)\n",
    "    best_score = score\n",
    "    \n",
    "    y_true_2, y_pred_2 = [], []\n",
    "    for step, ((sst, t300, ua, va), label) in enumerate(valid_loader_2):\n",
    "        sst = sst.to(device).float()\n",
    "        t300 = t300.to(device).float()\n",
    "        ua = ua.to(device).float()\n",
    "        va = va.to(device).float()\n",
    "        label = label.to(device).float()\n",
    "        preds = model(sst, t300, ua, va)\n",
    "\n",
    "        y_pred_2.append(preds)\n",
    "        y_true_2.append(label)\n",
    "\n",
    "    y_true_2 = torch.cat(y_true_2, axis=0).cpu().detach().numpy()\n",
    "    y_pred_2 = torch.cat(y_pred_2, axis=0).cpu().detach().numpy()\n",
    "    x_month = np.arange(24)\n",
    "    score_2 = eval_score(y_true_2, y_pred_2)\n",
    "    print('Epoch: {}, Valid Score: {}, Valid Score 2: {}\\n'.format(epoch+1,score,score_2))    \n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        all_loss = []\n",
    "        for step, ((sst, t300, ua, va), label) in enumerate(train_loader):                \n",
    "            sst = sst.to(device).float()\n",
    "            t300 = t300.to(device).float()\n",
    "            ua = ua.to(device).float()\n",
    "            va = va.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            label = label.to(device).float()\n",
    "            preds = model(sst, t300, ua, va)\n",
    "            loss = loss_fn(preds, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            all_loss.append(loss.item())\n",
    "            if step%20 == 0:\n",
    "                print('Step: {}, Train Loss: {}'.format(step, loss))\n",
    "        print('Epoch: {}, Train loss: {}'.format(epoch+1, np.mean(all_loss)))\n",
    "        loss_epoch.append(np.mean(all_loss))\n",
    "\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        for step, ((sst, t300, ua, va), label) in enumerate(valid_loader):\n",
    "            sst = sst.to(device).float()\n",
    "            t300 = t300.to(device).float()\n",
    "            ua = ua.to(device).float()\n",
    "            va = va.to(device).float()\n",
    "            label = label.to(device).float()\n",
    "            preds = model(sst, t300, ua, va)\n",
    "\n",
    "            y_pred.append(preds)\n",
    "            y_true.append(label)\n",
    "\n",
    "        y_true = torch.cat(y_true, axis=0).cpu().detach().numpy()\n",
    "        y_pred = torch.cat(y_pred, axis=0).cpu().detach().numpy()\n",
    "        x_month = np.arange(24)\n",
    "        score = eval_score(y_true, y_pred)\n",
    "        score_epoch.append(score)\n",
    "        \n",
    "        y_true_2, y_pred_2 = [], []\n",
    "        for step, ((sst, t300, ua, va), label) in enumerate(valid_loader_2):\n",
    "            sst = sst.to(device).float()\n",
    "            t300 = t300.to(device).float()\n",
    "            ua = ua.to(device).float()\n",
    "            va = va.to(device).float()\n",
    "            label = label.to(device).float()\n",
    "            preds = model(sst, t300, ua, va)\n",
    "\n",
    "            y_pred_2.append(preds)\n",
    "            y_true_2.append(label)\n",
    "\n",
    "        y_true_2 = torch.cat(y_true_2, axis=0).cpu().detach().numpy()\n",
    "        y_pred_2 = torch.cat(y_pred_2, axis=0).cpu().detach().numpy()\n",
    "        x_month = np.arange(24)\n",
    "        score_2 = eval_score(y_true_2, y_pred_2)\n",
    "        score_epoch_2.append(score_2)\n",
    "        print('Epoch: {}, Valid Score: {}, Valid Score 2: {}\\n'.format(epoch+1,score,score_2))    \n",
    "        \n",
    "        torch.save(model.state_dict(), './models/basemodel_epoch_{}.pt'.format(epoch+1))\n",
    "        if score > best_score:\n",
    "            torch.save(model.state_dict(), './models/basemodel_best.pt')\n",
    "            print('Model saved successfully')\n",
    "            best_score = score\n",
    "            \n",
    "        # figure\n",
    "        plt.figure(figsize = (10,5))\n",
    "        for i in range(10):\n",
    "            plt.subplot(5,5,i+1)\n",
    "            plt.plot(x_month, y_true[i],color='red')\n",
    "            plt.plot(x_month, y_pred[i],color='blue')\n",
    "        for i in range(10, 23):\n",
    "            plt.subplot(5,5,i+1)\n",
    "            plt.plot(x_month, y_true_2[i],color='red')\n",
    "            plt.plot(x_month, y_pred_2[i],color='blue')\n",
    "        plt.subplot(5,5,24)\n",
    "        plt.plot(np.arange(len(loss_epoch))[:20],loss_epoch[-20:])\n",
    "        plt.subplot(5,5,25)\n",
    "        plt.plot(np.arange(len(score_epoch)),score_epoch)\n",
    "        plt.plot(np.arange(len(score_epoch)),score_epoch_2)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    max_score = max(score_epoch)\n",
    "    max_epoch = score_epoch.index(max_score) + 1\n",
    "    print(\"max score: {} at eopch {}\".format(max_score, max_epoch))\n",
    "    max_score_2 = max(score_epoch_2)\n",
    "    max_epoch_2 = score_epoch_2.index(max_score_2) + 1\n",
    "    print(\"max score 2: {} at eopch {}\".format(max_score_2, max_epoch_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        self.conv = nn.Sequential(nn.Conv3d(in_channels=4, out_channels=64, kernel_size=(3,4,8)),\n",
    "                                  nn.ReLU(inplace=True),\n",
    "                                  nn.BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True),\n",
    "                                  nn.Conv3d(in_channels=64, out_channels=128, kernel_size=(3,4,8)),\n",
    "                                  nn.MaxPool3d(kernel_size=(1,2,2), stride=(1,2,2)),\n",
    "                                  nn.ReLU(inplace=True),\n",
    "                                  nn.BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True),\n",
    "                                  nn.Conv3d(in_channels=128, out_channels=256, kernel_size=(3,4,8)),\n",
    "                                  nn.ReLU(inplace=True),\n",
    "                                  nn.BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True),\n",
    "                                  nn.AdaptiveAvgPool3d(1)) \n",
    "        \n",
    "        self.linear0 = nn.Linear(256, 64)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.linear = nn.Linear(64, 24)\n",
    "\n",
    "    def forward(self, sst, t300, ua, va):\n",
    "        \n",
    "        x = torch.stack([sst, t300, ua, va], dim=1) # batch * 4 * 12 * 24 * 72\n",
    "        bs = x.shape[0]\n",
    "        x = self.conv(x).view(bs, -1)\n",
    "        x = self.linear0(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_Model()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'   \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "gpu_ids = [i for i in range(int(torch.cuda.device_count()))]\n",
    "model = torch.nn.DataParallel(model.to(\"cuda:0\"), device_ids=gpu_ids)\n",
    "loss_fn = loss_fn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel : all params: 3.976088M\n"
     ]
    }
   ],
   "source": [
    "print('{} : all params: {:4f}M'.format(model._get_name(), sum(p.numel() for p in model.parameters()) / 1000 / 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): CNN_Model(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv3d(4, 64, kernel_size=(3, 4, 8), stride=(1, 1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Conv3d(64, 128, kernel_size=(3, 4, 8), stride=(1, 1, 1))\n",
       "      (4): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (7): Conv3d(128, 256, kernel_size=(3, 4, 8), stride=(1, 1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): AdaptiveAvgPool3d(output_size=1)\n",
       "    )\n",
       "    (linear0): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (tanh): Tanh()\n",
       "    (linear): Linear(in_features=64, out_features=24, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('models/basemodel_best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = './tcdata/enso_round1_test_20210201/'\n",
    "\n",
    "### load test data\n",
    "files = os.listdir(test_path)\n",
    "test_feas_dict = {}\n",
    "for file in files:\n",
    "    test_feas_dict[file] = np.load(test_path + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. predict\n",
    "test_predicts_dict = {}\n",
    "for file_name, val in test_feas_dict.items():\n",
    "    SST = np.expand_dims(val[:,:,:,0],axis=0)\n",
    "    T300 = np.expand_dims(val[:,:,:,1],axis=0)\n",
    "    Ua = np.expand_dims(val[:,:,:,2],axis=0)\n",
    "    Va = np.expand_dims(val[:,:,:,3],axis=0)\n",
    "    \n",
    "    SST = np.nan_to_num(SST) # trans nan to 0\n",
    "    T300 = np.nan_to_num(T300)\n",
    "    Ua = np.nan_to_num(Ua)\n",
    "    Va = np.nan_to_num(Va)\n",
    "    \n",
    "#     data_list = [SST,T300,Ua,Va]\n",
    "#     SST,T300,Ua,Va = fit_data(data_list, fit=False)\n",
    "\n",
    "    SST = torch.tensor(SST).to(device).float()\n",
    "    T300 = torch.tensor(T300).to(device).float()\n",
    "    Ua = torch.tensor(Ua).to(device).float()\n",
    "    Va = torch.tensor(Va).to(device).float()\n",
    "    \n",
    "    result = model(SST, T300, Ua, Va).view(-1).detach().cpu().numpy()\n",
    "    test_predicts_dict[file_name] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. save results\n",
    "if os.path.exists('./result/'):  \n",
    "    shutil.rmtree('./result/', ignore_errors=True)  \n",
    "os.makedirs('./result/')\n",
    "for file_name, val in test_predicts_dict.items(): \n",
    "    np.save('./result/' + file_name, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_zip(res_dir='./result', output_dir='result.zip'):  \n",
    "    z = zipfile.ZipFile(output_dir, 'w')  \n",
    "    for file in os.listdir(res_dir):  \n",
    "        if '.npy' not in file:\n",
    "            continue\n",
    "        z.write(res_dir + os.sep + file)  \n",
    "    z.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
