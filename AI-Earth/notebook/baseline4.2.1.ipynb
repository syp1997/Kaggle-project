{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.models as models\n",
    "from sklearn import preprocessing\n",
    "import zipfile\n",
    "import shutil\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'   \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 427):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarthDataSet(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['sst'])\n",
    "\n",
    "    def __getitem__(self, idx):   \n",
    "        return (self.data['sst'][idx], self.data['t300'][idx], self.data['ua'][idx], self.data['va'][idx]), self.data['label'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_data(data_list, fit=True):\n",
    "    a,b,c,d = data_list[0].shape\n",
    "    all_data = []\n",
    "    for data in data_list:\n",
    "        new_data = data.reshape(-1)\n",
    "        all_data.append(new_data)\n",
    "    all_data = np.stack(all_data,1)\n",
    "    print(all_data.shape)\n",
    "    if fit:\n",
    "        standardScaler.fit(all_data)\n",
    "        print(\"fit train data\")\n",
    "    all_data = standardScaler.transform(all_data)\n",
    "    res_data = []\n",
    "    for i in range(all_data.shape[1]):\n",
    "        data = all_data[:,i].reshape(a,b,c,d)\n",
    "        res_data.append(data)\n",
    "    return res_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # CMIP data    \n",
    "    train = xr.open_dataset('tcdata/enso_round1_train_20210201/CMIP_train.nc')\n",
    "    label = xr.open_dataset('tcdata/enso_round1_train_20210201/CMIP_label.nc')    \n",
    "   \n",
    "    train_sst = train['sst'][:, :12].values.astype('float64')  # (4645, 12, 24, 72)\n",
    "    train_t300 = train['t300'][:, :12].values.astype('float64')\n",
    "    train_ua = train['ua'][:, :12].values.astype('float64')\n",
    "    train_va = train['va'][:, :12].values.astype('float64')\n",
    "    train_label = label['nino'][:, 12:36].values.astype('float64')\n",
    "\n",
    "    train_ua = np.nan_to_num(train_ua) # trans nan to 0\n",
    "    train_va = np.nan_to_num(train_va)\n",
    "    train_t300 = np.nan_to_num(train_t300)\n",
    "    train_sst = np.nan_to_num(train_sst)\n",
    "    \n",
    "#     data_list = [train_sst,train_t300,train_ua,train_va]\n",
    "#     train_sst,train_t300,train_ua,train_va = fit_data(data_list, fit=True)\n",
    "\n",
    "    # SODA data    \n",
    "    train2 = xr.open_dataset('tcdata/enso_round1_train_20210201/SODA_train.nc')\n",
    "    label2 = xr.open_dataset('tcdata/enso_round1_train_20210201/SODA_label.nc')\n",
    "    \n",
    "    train_sst2 = train2['sst'][:, :12].values.astype('float64')  # (100, 12, 24, 72)\n",
    "    train_t3002 = train2['t300'][:, :12].values.astype('float64')\n",
    "    train_ua2 = train2['ua'][:, :12].values.astype('float64')\n",
    "    train_va2 = train2['va'][:, :12].values.astype('float64')\n",
    "    train_label2 = label2['nino'][:, 12:36].values.astype('float64')\n",
    "    \n",
    "    train_sst2 = np.nan_to_num(train_sst2) # trans nan to 0\n",
    "    train_t3002 = np.nan_to_num(train_t3002)\n",
    "    train_ua2 = np.nan_to_num(train_ua2)\n",
    "    train_va2 = np.nan_to_num(train_va2)\n",
    "    \n",
    "#     data_list = [train_sst2,train_t3002,train_ua2,train_va2]\n",
    "#     train_sst2,train_t3002,train_ua2,train_va2 = fit_data(data_list, fit=False)\n",
    "\n",
    "    dict_cmip = {\n",
    "        'sst':train_sst,\n",
    "        't300':train_t300,\n",
    "        'ua':train_ua,\n",
    "        'va': train_va,\n",
    "        'label': train_label}\n",
    "    dict_soda = {\n",
    "        'sst':train_sst2,\n",
    "        't300':train_t3002,\n",
    "        'ua':train_ua2,\n",
    "        'va': train_va2,\n",
    "        'label': train_label2}\n",
    "    \n",
    "    cmip_dataset = EarthDataSet(dict_cmip)\n",
    "    soda_dataset = EarthDataSet(dict_soda)\n",
    "    \n",
    "    train_1, valid_1 = random_split(cmip_dataset, [4545, 100])\n",
    "    train_2, valid_2 = random_split(soda_dataset, [0, 100])\n",
    "    \n",
    "    train_dataset = train_1 \n",
    "    valid_dataset = valid_1\n",
    "    valid_dataset_2 = valid_2\n",
    "    \n",
    "    print('Train samples: {}, Valid1 samples: {}, Valid2 samples: {}'.format(len(train_dataset), len(valid_dataset), len(valid_dataset_2)))\n",
    "    \n",
    "    return train_dataset, valid_dataset, valid_dataset_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 4545, Valid1 samples: 100, Valid2 samples: 100\n"
     ]
    }
   ],
   "source": [
    "set_seed()\n",
    "standardScaler = StandardScaler()\n",
    "train_dataset, valid_dataset, valid_dataset_2 = load_data()      \n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "valid_loader_2 = DataLoader(valid_dataset_2, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coreff(x, y):\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    c1 = sum((x - x_mean) * (y - y_mean))\n",
    "    c2 = sum((x - x_mean)**2) * sum((y - y_mean)**2)\n",
    "    return c1/np.sqrt(c2)\n",
    "\n",
    "def rmse(preds, y):\n",
    "    r = np.sqrt(sum((preds - y)**2) / preds.shape[0])\n",
    "    return r\n",
    "\n",
    "def eval_score(preds, label):\n",
    "    acskill_socre = 0\n",
    "    rmse_score = 0\n",
    "    a = [1.5]*4 + [2]*7 + [3]*7 + [4]*6\n",
    "    for i in range(24):\n",
    "        r = rmse(preds[:, i], label[:, i], ) # T时刻 (100,)\n",
    "        cor = coreff(preds[:, i], label[:, i], )\n",
    "    \n",
    "        rmse_score += r\n",
    "        acskill_socre += a[i] * np.log(i+1) * cor\n",
    "    print(\"acskill_socre:{}, rmse_score:{}\".format(2/3*acskill_socre, rmse_score))\n",
    "    return 2/3 * acskill_socre - rmse_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    best_score = -99\n",
    "    loss_epoch = []\n",
    "    score_epoch = []\n",
    "    score_epoch_2 = []\n",
    "    epoch = -1\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for step, ((sst, t300, ua, va), label) in enumerate(valid_loader):\n",
    "        sst = sst.to(device).float()\n",
    "        t300 = t300.to(device).float()\n",
    "        ua = ua.to(device).float()\n",
    "        va = va.to(device).float()\n",
    "        label = label.to(device).float()\n",
    "        preds = model(sst, t300, ua, va)\n",
    "\n",
    "        y_pred.append(preds)\n",
    "        y_true.append(label)\n",
    "\n",
    "    y_true = torch.cat(y_true, axis=0).cpu().detach().numpy()\n",
    "    y_pred = torch.cat(y_pred, axis=0).cpu().detach().numpy()\n",
    "    x_month = np.arange(24)\n",
    "    score = eval_score(y_true, y_pred)\n",
    "    best_score = score\n",
    "    \n",
    "    y_true_2, y_pred_2 = [], []\n",
    "    for step, ((sst, t300, ua, va), label) in enumerate(valid_loader_2):\n",
    "        sst = sst.to(device).float()\n",
    "        t300 = t300.to(device).float()\n",
    "        ua = ua.to(device).float()\n",
    "        va = va.to(device).float()\n",
    "        label = label.to(device).float()\n",
    "        preds = model(sst, t300, ua, va)\n",
    "\n",
    "        y_pred_2.append(preds)\n",
    "        y_true_2.append(label)\n",
    "\n",
    "    y_true_2 = torch.cat(y_true_2, axis=0).cpu().detach().numpy()\n",
    "    y_pred_2 = torch.cat(y_pred_2, axis=0).cpu().detach().numpy()\n",
    "    x_month = np.arange(24)\n",
    "    score_2 = eval_score(y_true_2, y_pred_2)\n",
    "    print('Epoch: {}, Valid Score: {}, Valid Score 2: {}\\n'.format(epoch+1,score,score_2))    \n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        all_loss = []\n",
    "        for step, ((sst, t300, ua, va), label) in enumerate(train_loader):                \n",
    "            sst = sst.to(device).float()\n",
    "            t300 = t300.to(device).float()\n",
    "            ua = ua.to(device).float()\n",
    "            va = va.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            label = label.to(device).float()\n",
    "            preds = model(sst, t300, ua, va)\n",
    "            loss = loss_fn(preds, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            all_loss.append(loss.item())\n",
    "            if step%20 == 0:\n",
    "                print('Step: {}, Train Loss: {}'.format(step, loss))\n",
    "        print('Epoch: {}, Train loss: {}'.format(epoch+1, np.mean(all_loss)))\n",
    "        loss_epoch.append(np.mean(all_loss))\n",
    "\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        for step, ((sst, t300, ua, va), label) in enumerate(valid_loader):\n",
    "            sst = sst.to(device).float()\n",
    "            t300 = t300.to(device).float()\n",
    "            ua = ua.to(device).float()\n",
    "            va = va.to(device).float()\n",
    "            label = label.to(device).float()\n",
    "            preds = model(sst, t300, ua, va)\n",
    "\n",
    "            y_pred.append(preds)\n",
    "            y_true.append(label)\n",
    "\n",
    "        y_true = torch.cat(y_true, axis=0).cpu().detach().numpy()\n",
    "        y_pred = torch.cat(y_pred, axis=0).cpu().detach().numpy()\n",
    "        x_month = np.arange(24)\n",
    "        score = eval_score(y_true, y_pred)\n",
    "        score_epoch.append(score)\n",
    "        \n",
    "        y_true_2, y_pred_2 = [], []\n",
    "        for step, ((sst, t300, ua, va), label) in enumerate(valid_loader_2):\n",
    "            sst = sst.to(device).float()\n",
    "            t300 = t300.to(device).float()\n",
    "            ua = ua.to(device).float()\n",
    "            va = va.to(device).float()\n",
    "            label = label.to(device).float()\n",
    "            preds = model(sst, t300, ua, va)\n",
    "\n",
    "            y_pred_2.append(preds)\n",
    "            y_true_2.append(label)\n",
    "\n",
    "        y_true_2 = torch.cat(y_true_2, axis=0).cpu().detach().numpy()\n",
    "        y_pred_2 = torch.cat(y_pred_2, axis=0).cpu().detach().numpy()\n",
    "        x_month = np.arange(24)\n",
    "        score_2 = eval_score(y_true_2, y_pred_2)\n",
    "        score_epoch_2.append(score_2)\n",
    "        print('Epoch: {}, Valid Score: {}, Valid Score 2: {}\\n'.format(epoch+1,score,score_2))    \n",
    "        \n",
    "        torch.save(model.state_dict(), './models/basemodel_epoch_{}.pt'.format(epoch+1))\n",
    "        if score > best_score:\n",
    "            torch.save(model.state_dict(), './models/basemodel_best.pt')\n",
    "            print('Model saved successfully')\n",
    "            best_score = score\n",
    "            \n",
    "        # figure\n",
    "        plt.figure(figsize = (10,5))\n",
    "        for i in range(10):\n",
    "            plt.subplot(5,5,i+1)\n",
    "            plt.plot(x_month, y_true[i],color='red')\n",
    "            plt.plot(x_month, y_pred[i],color='blue')\n",
    "        for i in range(10, 23):\n",
    "            plt.subplot(5,5,i+1)\n",
    "            plt.plot(x_month, y_true_2[i],color='red')\n",
    "            plt.plot(x_month, y_pred_2[i],color='blue')\n",
    "        plt.subplot(5,5,24)\n",
    "        plt.plot(np.arange(len(loss_epoch))[:20],loss_epoch[-20:])\n",
    "        plt.subplot(5,5,25)\n",
    "        plt.plot(np.arange(len(score_epoch)),score_epoch)\n",
    "        plt.plot(np.arange(len(score_epoch)),score_epoch_2)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    max_score = max(score_epoch)\n",
    "    max_epoch = score_epoch.index(max_score) + 1\n",
    "    print(\"max score: {} at eopch {}\".format(max_score, max_epoch))\n",
    "    max_score_2 = max(score_epoch_2)\n",
    "    max_epoch_2 = score_epoch_2.index(max_score_2) + 1\n",
    "    print(\"max score 2: {} at eopch {}\".format(max_score_2, max_epoch_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
    "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
    "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",\n",
    "                      stacklevel=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Values are generated by using a truncated uniform distribution and\n",
    "        # then using the inverse CDF for the normal distribution.\n",
    "        # Get upper and lower cdf values\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        # Uniformly fill tensor with values from [l, u], then translate to\n",
    "        # [2l-1, 2u-1].\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "\n",
    "        # Use inverse cdf transform for normal distribution to get truncated\n",
    "        # standard normal\n",
    "        tensor.erfinv_()\n",
    "\n",
    "        # Transform to proper mean, std\n",
    "        tensor.mul_(std * math.sqrt(2.))\n",
    "        tensor.add_(mean)\n",
    "\n",
    "        # Clamp to ensure it's in the proper range\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    # type: (Tensor, float, float, float, float) -> Tensor\n",
    "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
    "    normal distribution. The values are effectively drawn from the\n",
    "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
    "    with values outside :math:`[a, b]` redrawn until they are within\n",
    "    the bounds. The method used for generating the random values works\n",
    "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
    "    Args:\n",
    "        tensor: an n-dimensional `torch.Tensor`\n",
    "        mean: the mean of the normal distribution\n",
    "        std: the standard deviation of the normal distribution\n",
    "        a: the minimum cutoff value\n",
    "        b: the maximum cutoff value\n",
    "    Examples:\n",
    "        >>> w = torch.empty(3, 5)\n",
    "        >>> nn.init.trunc_normal_(w)\n",
    "    \"\"\"\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"\"\" Original Implementation of the gelu activation function in Google Bert repo when initialy created.\n",
    "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
    "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "        Also see https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=gelu, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.mat = torch.matmul\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (self.mat(q, k.transpose(-2, -1))) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = self.mat(attn, v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "    \n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, proj_drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=gelu, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=proj_drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "#         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Model, self).__init__()\n",
    "\n",
    "#         resnet = models.resnet18()\n",
    "#         resnet.conv1 = nn.Conv2d(4, 64, kernel_size=(4, 8), stride=(1, 1), padding=(2, 4), bias=False)\n",
    "#         resnet.maxpool = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "#         self.cnn = nn.Sequential(*(list(resnet.children())[:-1]))\n",
    "        \n",
    "        self.cnn = nn.Sequential(nn.Conv2d(48, 48, kernel_size=(4, 8), stride=(1, 1), bias=False),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.Conv2d(48, 48, kernel_size=(2, 4), stride=(1, 1), bias=False),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.Conv2d(48, 48, kernel_size=(2, 4), stride=(1, 1), bias=False),\n",
    "                                   nn.BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True),\n",
    "                                   nn.ReLU(inplace=True),)\n",
    "        \n",
    "        embed_dim = 384\n",
    "        self.embed = nn.Linear(1121, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 49, embed_dim))\n",
    "        \n",
    "        self.block = nn.Sequential(Block(dim=embed_dim, num_heads=8),\n",
    "                                   Block(dim=embed_dim, num_heads=8),\n",
    "                                   Block(dim=embed_dim, num_heads=8),)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d(1,256)\n",
    "        \n",
    "        # Classifier head\n",
    "        self.linear = nn.Linear(embed_dim, 64)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.head = nn.Linear(64, 24)\n",
    "    \n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        \n",
    "    def forward_features(self, x):\n",
    "        bs = x.shape[0]\n",
    "        cls_tokens = self.cls_token.expand(bs, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.block(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x[:,0]\n",
    "\n",
    "    def forward(self, sst, t300, ua, va):\n",
    "        x = torch.cat([sst, t300, ua, va], dim=1) # b * 48 * 24 * 72\n",
    "        x = self.cnn(x)\n",
    "        bs = x.shape[0]\n",
    "        x = x.view(bs, 48, -1)\n",
    "        \n",
    "        x = self.embed(x)\n",
    "        x = self.forward_features(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.head(x)  # b * 24\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_Model()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'   \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "gpu_ids = [i for i in range(int(torch.cuda.device_count()))]\n",
    "model = torch.nn.DataParallel(model.to(\"cuda:0\"), device_ids=gpu_ids)\n",
    "loss_fn = loss_fn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## epoch 14: score 28, 28  3 layers 1e-4\n",
    "## epoch 15: score 25, 17  3 layers 1e-4\n",
    "## epoch 23: score 27, 7   2 layers 1e-4\n",
    "\n",
    "## epoch 7 : score 29, 13  6 layers 1e-4\n",
    "## epoch 13 : score 22, 9  6 layers 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel : all params: 5.907640M\n"
     ]
    }
   ],
   "source": [
    "print('{} : all params: {:4f}M'.format(model._get_name(), sum(p.numel() for p in model.parameters()) / 1000 / 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): CNN_Model(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(48, 48, kernel_size=(4, 8), stride=(1, 1), bias=False)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(48, 48, kernel_size=(2, 4), stride=(1, 1), bias=False)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(48, 48, kernel_size=(2, 4), stride=(1, 1), bias=False)\n",
       "      (5): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "    )\n",
       "    (embed): Linear(in_features=1121, out_features=384, bias=True)\n",
       "    (block): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    (linear): Linear(in_features=384, out_features=64, bias=True)\n",
       "    (tanh): Tanh()\n",
       "    (head): Linear(in_features=64, out_features=24, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('models/basemodel_best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = './tcdata/enso_round1_test_20210201/'\n",
    "\n",
    "### load test data\n",
    "files = os.listdir(test_path)\n",
    "test_feas_dict = {}\n",
    "for file in files:\n",
    "    test_feas_dict[file] = np.load(test_path + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. predict\n",
    "test_predicts_dict = {}\n",
    "for file_name, val in test_feas_dict.items():\n",
    "    SST = np.expand_dims(val[:,:,:,0],axis=0)\n",
    "    T300 = np.expand_dims(val[:,:,:,1],axis=0)\n",
    "    Ua = np.expand_dims(val[:,:,:,2],axis=0)\n",
    "    Va = np.expand_dims(val[:,:,:,3],axis=0)\n",
    "    \n",
    "    SST = np.nan_to_num(SST) # trans nan to 0\n",
    "    T300 = np.nan_to_num(T300)\n",
    "    Ua = np.nan_to_num(Ua)\n",
    "    Va = np.nan_to_num(Va)\n",
    "    \n",
    "#     data_list = [SST,T300,Ua,Va]\n",
    "#     SST,T300,Ua,Va = fit_data(data_list, fit=False)\n",
    "\n",
    "    SST = torch.tensor(SST).to(device).float()\n",
    "    T300 = torch.tensor(T300).to(device).float()\n",
    "    Ua = torch.tensor(Ua).to(device).float()\n",
    "    Va = torch.tensor(Va).to(device).float()\n",
    "    \n",
    "    result = model(SST, T300, Ua, Va).view(-1).detach().cpu().numpy()\n",
    "    test_predicts_dict[file_name] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. save results\n",
    "if os.path.exists('./result/'):  \n",
    "    shutil.rmtree('./result/', ignore_errors=True)  \n",
    "os.makedirs('./result/')\n",
    "for file_name, val in test_predicts_dict.items(): \n",
    "    np.save('./result/' + file_name, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_zip(res_dir='./result', output_dir='result.zip'):  \n",
    "    z = zipfile.ZipFile(output_dir, 'w')  \n",
    "    for file in os.listdir(res_dir):  \n",
    "        if '.npy' not in file:\n",
    "            continue\n",
    "        z.write(res_dir + os.sep + file)  \n",
    "    z.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
